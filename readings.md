#Readings in Safe and Robust Machine Learning

##Position Papers and Articles
1. [Concrete Problems in AI Safety](https://arxiv.org/pdf/1606.06565v1.pdf)
2. [Machine Learning: The High-Interest Credit Card of Technical Debt](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43146.pdf)
3. [Cyber-physical systems you can bet your life on](https://www.microsoft.com/en-us/research/cyber-physical-systems-can-bet-life/)

##Technical Papers
There are a wide range of papers discussing the safty and robustness of machine learning in specific applications domains as well as techniques to address them. The following is undoutedly an incomplete list, and is getting constantly updated. Let me know if you know of a relavant paper!

1. [Intriguing Properties of Neural Networks](https://cs.nyu.edu/~zaremba/docs/understanding.pdf)
2. [Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images](http://arxiv.org/pdf/1412.1897v4.pdf)
3. [Improving the Robustness of Deep Neural Networks via Stability Training](http://arxiv.org/pdf/1604.04326v1.pdf)
4. [Towards Deep Neural Network Architectures Robust to Adversarial Examples](http://arxiv.org/pdf/1412.5068v4.pdf)
5. [Measuring Neural Net Robustness with Constraints](http://arxiv.org/pdf/1605.07262v1.pdf)
6. [Adversarial Manipulation of Deep Representations](https://arxiv.org/abs/1511.05122)
7. [Exploring the Space of Adversarial Images](https://arxiv.org/abs/1510.05328)
8. [Ensemble Robustness of Deep Learning Algorithms](https://arxiv.org/abs/1602.02389)
9. [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)
10. [Learning with a Strong Adversary](http://arxiv.org/abs/1511.03034)
11. [Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization](http://arxiv.org/abs/1511.05432)
12. [Robustness and Generalization](http://arxiv.org/abs/1005.2243)
13. [Analysis of classifiers' robustness to adversarial perturbations](http://arxiv.org/abs/1502.02590)
14. [Nightmare at test time: robust learning by feature deletion](https://www.cs.nyu.edu/~roweis/papers/robust_icml06.pdf)
